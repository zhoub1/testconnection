import re
from aws_cdk import (
    Duration, Aws, Stack, RemovalPolicy, CfnOutput, CfnDeletionPolicy,
    aws_iam as iam,
    aws_s3 as s3,
    aws_lambda as _lambda,
    aws_s3_notifications as s3_notifications,
    aws_glue as glue,
    aws_lakeformation as lakeformation,
    aws_datazone as datazone,
    Tags
)
from constructs import Construct
import json
import boto3

class CdkDatazoneStack(Stack):
    r"""               
    Authors: Bill Zhou & Justin Miles
    Project: Leidos | AWS SCA (ConnectiV)

    CDK Stack to provision AWS DataZone Domain, Project, and Environment along with necessary resources.

    This stack provisions the following resources:
    - AWS DataZone Domain, Project, and Environment
    - S3 buckets for data storage
    - Glue Crawler for processing data
    - IAM roles with necessary permissions for all services
    - Configures proper deletion of all resources, including revocation of Lake Formation permissions

    Attributes:
        scope (Construct): The scope in which this construct is defined.
        construct_id (str): The scoped construct ID.
        **kwargs: Additional keyword arguments.
    """

    def __init__(self, scope: Construct, construct_id: str, **kwargs) -> None:
        super().__init__(scope, construct_id, **kwargs)
        
        r"""
        Configure each parameter for your particular use case in the context file (cdk.json)
        Parameter details:
        * **domain_name (String):** A unique name for the DataZone domain. Must contain only alphanumeric characters.
        * **domain_description (String):** A description of the DataZone domain providing context about its purpose.
        * **project_name (String):** The name of the DataZone project. Must contain only lowercase alphanumeric characters.
        * **environment_name (String):** The name of the DataZone environment. Must be composed entirely of lowercase letters.
        * **environment_profile_name (String):** The name of the environment profile. Defines the configuration for the environment.
        * **glue_crawler_schedule (String, Default: "cron(0 8 * * ? *)"):** Schedule for the Glue Crawler.
        * **data_source_schedule (String, Default: "cron(0 9 * * ? *)"):** Schedule for created data source ingestion.
        """
        # Fetching parameters from context
        domain_name = self.node.try_get_context("domain_name") or "datazonedomaincdk"
        domain_description = self.node.try_get_context("domain_description") or "Description of the DataZone domain (generated by CDK)"
        project_name = self.node.try_get_context("project_name") or "datazoneproj"
        environment_name = self.node.try_get_context("environment_name") or "datazoneenv"
        environment_profile_name = self.node.try_get_context("environment_profile_name") or "env_profile"
        glue_crawler_schedule = self.node.try_get_context("glue_crawler_schedule") or "cron(0 8 * * ? *)"
        data_source_schedule = self.node.try_get_context("data_source_schedule") or "cron(0 9 * * ? *)"

        # Helper functions for name sanitization
        def sanitize_name(name, pattern, max_length, allowed_chars, name_type, description):
            """
            Validates and sanitizes a name based on the provided regex pattern and allowed characters.

            Args:
                name (str): The name to validate and sanitize.
                pattern (str): The regex pattern the name must match.
                max_length (int): The maximum allowed length of the name.
                allowed_chars (str): A string of allowed characters.
                name_type (str): The type of name (used in error messages).
                description (str): Description of the naming rules (used in error messages).

            Returns:
                str: The sanitized name, truncated to max_length.

            Raises:
                ValueError: If the name does not match the pattern.
            """
            if not re.match(pattern, name):
                raise ValueError(f"Invalid {name_type} '{name}'. {description}")
            sanitized = ''.join(c if c in allowed_chars else '-' for c in name)
            return sanitized[:max_length]

        def sanitize_bucket_name(name):
            # Bucket names must be between 3 and 63 characters long
            # Must be lowercase letters, numbers, and hyphens
            # Must start and end with a letter or number
            name = name.lower()
            name = re.sub(r'[^a-z0-9-]', '-', name)
            name = re.sub(r'^-+', '', name)
            name = re.sub(r'-+$', '', name)
            if not (3 <= len(name) <= 63):
                raise ValueError(
                    f"S3 bucket name '{name}' must be between 3 and 63 characters long.\n"
                    f"Check the 'project_name' parameter as it affects the bucket name length"
                )
            return name

        # Validate parameters
        def validate_parameters():
            errors = []

            # Validate domain_name
            if not re.match(r'^[A-Za-z0-9]+$', domain_name):
                errors.append(f"Invalid domain name '{domain_name}'. Domain names can only contain letters and numbers (no special characters or spaces). Please update the 'domain_name' parameter in your context.")

            # Validate project_name
            if not re.match(r'^[a-z0-9]+$', project_name):
                errors.append(f"Invalid project name '{project_name}'. Project names can only contain lowercase letters and numbers (no uppercase letters, special characters, or spaces). Please update the 'project_name' parameter in your context.")

            # Validate environment_name
            if not re.match(r'^[a-z]+$', environment_name):
                errors.append(f"Invalid environment name '{environment_name}'. Environment names can only contain lowercase letters (no numbers, special characters, or spaces). Please update the 'environment_name' parameter in your context.")

            # Validate environment_profile_name
            if not re.match(r'^[A-Za-z0-9_-]+$', environment_profile_name):
                errors.append(f"Invalid environment profile name '{environment_profile_name}'. Environment profile names can only contain letters, numbers, underscores, and hyphens (no spaces or other special characters). Please update the 'environment_profile_name' parameter in your context.")

            # Validate glue_crawler_schedule and data_source_schedule
            cron_pattern = r'^cron\(.+\)$'
            if not re.match(cron_pattern, glue_crawler_schedule):
                errors.append(f"Invalid Glue Crawler schedule '{glue_crawler_schedule}'. Schedules must be in cron expression format. Please update the 'glue_crawler_schedule' parameter in your context.")

            if not re.match(cron_pattern, data_source_schedule):
                errors.append(f"Invalid Data Source schedule '{data_source_schedule}'. Schedules must be in cron expression format. Please update the 'data_source_schedule' parameter in your context.")

            if errors:
                error_message = "\n".join(errors)
                # Use CDK's node.add_error for validation errors
                self.node.add_error(f"Parameter validation failed:\n{error_message}")

        # Call the validate_parameters function
        validate_parameters()

        # Sanitize parameters
        domain_name = sanitize_name(
            domain_name,
            pattern=r'^[A-Za-z0-9]+$',
            max_length=64,
            allowed_chars='ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789',
            name_type='domain name',
            description='Domain names can only contain letters and numbers (no special characters or spaces). Please update the "domain_name" parameter in your context.'
        )

        project_name = sanitize_name(
            project_name,
            pattern=r'^[a-z0-9]+$',
            max_length=64,
            allowed_chars='abcdefghijklmnopqrstuvwxyz0123456789',
            name_type='project name',
            description='Project names can only contain lowercase letters and numbers (no uppercase letters, special characters, or spaces). Please update the "project_name" parameter in your context.'
        )

        environment_name = sanitize_name(
            environment_name,
            pattern=r'^[a-z]+$',
            max_length=64,
            allowed_chars='abcdefghijklmnopqrstuvwxyz',
            name_type='environment name',
            description='Environment names can only contain lowercase letters (no numbers, special characters, or spaces). Please update the "environment_name" parameter in your context.'
        )

        environment_profile_name = sanitize_name(
            environment_profile_name,
            pattern=r'^[A-Za-z0-9_-]+$',
            max_length=64,
            allowed_chars='ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789_-',
            name_type='environment profile name',
            description='Environment profile names can only contain letters, numbers, underscores, and hyphens (no spaces or other special characters). Please update the "environment_profile_name" parameter in your context.'
        )

        # Get the current user's ARN to add as initial project owner
        try:
            sts = boto3.client('sts')
            identity = sts.get_caller_identity()
            user_arn = identity['Arn']
        except Exception as e:
            self.node.add_error("Failed to retrieve user ARN using boto3. Ensure AWS credentials are configured and have the necessary permissions.")
            user_arn = "arn:aws:iam::123456789012:user/placeholder"

        # S3 Buckets for data source and DataZone system store
        s3_data_source_bucket_name = sanitize_bucket_name(f"datazone-src-{self.account}-{project_name}-{self.region}")
        s3_data_source = s3.Bucket(
            self,
            "S3DataSource",
            bucket_name=s3_data_source_bucket_name,
            encryption=s3.BucketEncryption.S3_MANAGED,
            versioned=True,
            removal_policy=RemovalPolicy.DESTROY,
            auto_delete_objects=True
        )
        Tags.of(s3_data_source).add("Project", project_name)
        Tags.of(s3_data_source).add("Environment", environment_name)

        s3_datazone_sys_bucket_name = sanitize_bucket_name(f"dzsysstore-{self.account}-{project_name}-{self.region}")
        s3_datazone_sys = s3.Bucket(
            self,
            "S3DataZoneSysStore",
            bucket_name=s3_datazone_sys_bucket_name,
            encryption=s3.BucketEncryption.S3_MANAGED,
            removal_policy=RemovalPolicy.DESTROY,
            auto_delete_objects=True
        )
        Tags.of(s3_datazone_sys).add("Project", project_name)
        Tags.of(s3_datazone_sys).add("Environment", environment_name)

        # IAM Role for Lambda execution with necessary permissions
        lambda_role_name = f"LambdaExecRole-{project_name}-{environment_name}"[:64]
        lambda_exec_role = iam.Role(
            self,
            "LambdaExecutionRole",
            role_name=lambda_role_name,
            assumed_by=iam.ServicePrincipal("lambda.amazonaws.com")
        )

        # Attach policies to the Lambda execution role
        lambda_exec_role.add_to_policy(iam.PolicyStatement(
            actions=["glue:StartCrawler"],
            resources=[
                self.format_arn(
                    service="glue",
                    resource="crawler",
                    resource_name=f"{project_name}_{environment_name}_pub_db_crawler"
                )
            ]
        ))
        lambda_exec_role.add_to_policy(iam.PolicyStatement(
            actions=[
                "logs:CreateLogGroup",
                "logs:CreateLogStream",
                "logs:PutLogEvents"
            ],
            resources=[f"arn:aws:logs:{self.region}:{self.account}:log-group:/aws/lambda/*"]
        ))

        # Lambda function to trigger Glue Crawler on S3 PUT event
        crawler_name = f"{project_name}_{environment_name}_pub_db_crawler"
        lambda_function_name = f"GlueCrawlerTriggerFunction-{project_name}-{environment_name}"[:64]
        glue_crawler_function = _lambda.Function(
            self,
            "GlueCrawlerTriggerFunction",
            function_name=lambda_function_name,
            runtime=_lambda.Runtime.NODEJS_16_X,
            handler="index.handler",
            code=_lambda.Code.from_inline(
                """
                const AWS = require('aws-sdk');
                const glue = new AWS.Glue();

                exports.handler = async (event) => {
                    // Structured logging of the received event
                    console.log(JSON.stringify({
                        level: 'INFO',
                        message: 'Received S3 event',
                        event: event
                    }));

                    const crawlerName = process.env.CRAWLER_NAME;
                    const params = { Name: crawlerName };

                    try {
                        // Attempt to start the Glue crawler
                        const data = await glue.startCrawler(params).promise();
                        console.log(JSON.stringify({
                            level: 'INFO',
                            message: `Glue crawler '${crawlerName}' started successfully.`,
                            data: data
                        }));
                    } catch (err) {
                        // Log error with detailed information
                        console.error(JSON.stringify({
                            level: 'ERROR',
                            message: `Failed to start Glue crawler '${crawlerName}'.`,
                            error: err.message,
                            stack: err.stack
                        }));
                        // Optionally rethrow the error if needed
                        throw err;
                    }
                };
                """
            ),
            environment={
                "CRAWLER_NAME": crawler_name
            },
            timeout=Duration.seconds(60),
            role=lambda_exec_role
        )

        # Permission for S3 to invoke the Lambda function
        glue_crawler_function.add_permission(
            "LambdaInvokePermission",
            principal=iam.ServicePrincipal("s3.amazonaws.com"),
            action="lambda:InvokeFunction",
            source_arn=s3_data_source.bucket_arn
        )

        # Add S3 notification to trigger Lambda on new object creation
        s3_data_source.add_event_notification(
            s3.EventType.OBJECT_CREATED,
            s3_notifications.LambdaDestination(glue_crawler_function)
        )

        # IAM Role for Amazon DataZone Domain Execution
        domain_role_name = f"DZDomainRole-{self.account}-{self.stack_name}"[:64]
        domain_exec_role = iam.Role(
            self,
            "DataZoneDomainExecutionRole",
            role_name=domain_role_name,
            assumed_by=iam.ServicePrincipal("datazone.amazonaws.com")
        )

        # Custom trust relationship
        domain_exec_role.assume_role_policy.add_statements(
            iam.PolicyStatement(
                effect=iam.Effect.ALLOW,
                actions=["sts:AssumeRole", "sts:TagSession"],
                principals=[
                    iam.ServicePrincipal("datazone.amazonaws.com"),
                    iam.ServicePrincipal("lakeformation.amazonaws.com")
                ],
                conditions={
                    "StringEquals": {
                        "aws:SourceAccount": Aws.ACCOUNT_ID
                    },
                    "ForAllValues:StringLike": {
                        "aws:TagKeys": "datazone*"
                    }
                }
            )
        )

        # Attach managed policies required by DataZone
        domain_exec_role.add_managed_policy(iam.ManagedPolicy.from_aws_managed_policy_name("AmazonAthenaFullAccess"))
        domain_exec_role.add_managed_policy(iam.ManagedPolicy.from_aws_managed_policy_name("service-role/AmazonDataZoneDomainExecutionRolePolicy"))
        domain_exec_role.add_managed_policy(iam.ManagedPolicy.from_aws_managed_policy_name("AmazonDataZoneRedshiftGlueProvisioningPolicy"))

        # Inline policy for Lake Formation permissions
        domain_exec_role.add_to_policy(iam.PolicyStatement(
            actions=[
                "lakeformation:GrantPermissions",
                "lakeformation:RevokePermissions",
                "lakeformation:GetDataAccess",
                "lakeformation:GetEffectivePermissionsForPath",
                "lakeformation:ListPermissions",
                "lakeformation:GetResourceLFTags",
                "lakeformation:PutDataLakeSettings",
                "lakeformation:DescribeResource",
                "lakeformation:RegisterResource",
                "lakeformation:DeregisterResource",
                "lakeformation:ListResources",
                "lakeformation:AddLFTagsToResource",
                "lakeformation:RemoveLFTagsFromResource",
                "lakeformation:SearchTablesByLFTags",
                "lakeformation:GetLFTag",
                "lakeformation:CreateLFTag",
                "lakeformation:DeleteLFTag",
                "lakeformation:ListLFTags",
                "lakeformation:TagResource",
                "lakeformation:UntagResource",
                "lakeformation:GetDataLakeSettings"
            ],
            resources=["*"]
        ))

        # Inline policy for IAM permissions
        domain_exec_role.add_to_policy(iam.PolicyStatement(
            actions=["iam:GetRole", "iam:GetUser"],
            resources=["*"]
        ))

        # Inline policy for logging
        domain_exec_role.add_to_policy(iam.PolicyStatement(
            actions=[
                "logs:CreateLogGroup",
                "logs:CreateLogStream",
                "logs:PutLogEvents"
            ],
            resources=[f"arn:aws:logs:{self.region}:{self.account}:log-group:/aws/lambda/*"]
        ))

        # Inline policy for specific S3 bucket access
        domain_exec_role.add_to_policy(iam.PolicyStatement(
            actions=[
                "s3:GetObject",
                "s3:PutObject",
                "s3:DeleteObject",
                "s3:ListBucket",
                "s3:PutBucketPolicy",
                "s3:GetBucketPolicy"
            ],
            resources=[
                s3_data_source.bucket_arn,
                f"{s3_data_source.bucket_arn}/*",
                s3_datazone_sys.bucket_arn,
                f"{s3_datazone_sys.bucket_arn}/*"
            ]
        ))

        # Tags for domain execution role
        Tags.of(domain_exec_role).add("Project", project_name)
        Tags.of(domain_exec_role).add("Environment", environment_name)

        # Lake Formation Data Lake Administrator settings
        data_lake_settings = lakeformation.CfnDataLakeSettings(
            self,
            "DataLakeAdministrator",
            admins=[lakeformation.CfnDataLakeSettings.DataLakePrincipalProperty(
                data_lake_principal_identifier=domain_exec_role.role_arn
            )],
            trusted_resource_owners=[domain_exec_role.role_arn]
        )
        data_lake_settings.node.add_dependency(domain_exec_role)

        # Amazon DataZone Domain creation
        datazone_domain = datazone.CfnDomain(
            self,
            "DataZoneDomain",
            name=domain_name,
            description=domain_description,
            domain_execution_role=domain_exec_role.role_arn
        )
        datazone_domain.node.add_dependency(domain_exec_role)

        # Amazon DataZone Project creation
        datazone_project = datazone.CfnProject(
            self,
            "DataZoneProject",
            name=project_name,
            description=domain_description,
            domain_identifier=datazone_domain.attr_id
        )
        datazone_project.node.add_dependency(datazone_domain)

        # Add the user as an owner of the DataZone project
        datazone_project_membership = datazone.CfnProjectMembership(
            self,
            "DataZoneProjectMembership",
            domain_identifier=datazone_domain.attr_id,
            project_identifier=datazone_project.attr_id,
            designation='PROJECT_OWNER',  # Valid values: 'PROJECT_OWNER', 'PROJECT_CONTRIBUTOR'
            member=datazone.CfnProjectMembership.MemberProperty(
                user_identifier=user_arn  # Use the current user's ARN
            )
        )
        datazone_project_membership.node.add_dependency(datazone_project)

        # Environment Blueprint configuration
        datazone_env_blueprint = datazone.CfnEnvironmentBlueprintConfiguration(
            self,
            "DataZoneEnvBlueprint",
            domain_identifier=datazone_domain.attr_id,
            environment_blueprint_identifier="DefaultDataLake",
            manage_access_role_arn=domain_exec_role.role_arn,
            provisioning_role_arn=domain_exec_role.role_arn,
            regional_parameters=[
                datazone.CfnEnvironmentBlueprintConfiguration.RegionalParameterProperty(
                    region=self.region,
                    parameters={"S3Location": f"s3://{s3_datazone_sys.bucket_name}"}
                )
            ],
            enabled_regions=[self.region]
        )
        datazone_env_blueprint.node.add_dependency(datazone_domain)

        # Environment Profile creation
        datazone_env_profile = datazone.CfnEnvironmentProfile(
            self,
            "DataZoneEnvProfile",
            name=environment_profile_name,
            project_identifier=datazone_project.attr_id,
            aws_account_id=self.account,
            aws_account_region=self.region,
            description=domain_description,
            domain_identifier=datazone_domain.attr_id,
            environment_blueprint_identifier=datazone_env_blueprint.attr_environment_blueprint_id
        )
        datazone_env_profile.node.add_dependency(datazone_env_blueprint)

        # Environment creation
        datazone_environment = datazone.CfnEnvironment(
            self,
            "DataZoneEnvironment",
            name=environment_name,
            domain_identifier=datazone_domain.attr_id,
            environment_profile_identifier=datazone_env_profile.attr_id,
            project_identifier=datazone_project.attr_id
        )
        datazone_environment.node.add_dependency(datazone_env_profile)
        # Add Deletion Policy to the DataZone environment
        datazone_environment.cfn_options.deletion_policy = CfnDeletionPolicy.DELETE

        # Data Source configuration
        datazone_data_source = datazone.CfnDataSource(
            self,
            "DataZoneDataSource",
            domain_identifier=datazone_domain.attr_id,
            environment_identifier=datazone_environment.attr_id,
            project_identifier=datazone_project.attr_id,
            name="PrimaryDataMeshDataSource",
            description="Data source for DataZone (Generated by CDK)",
            type="GLUE",
            configuration=datazone.CfnDataSource.DataSourceConfigurationInputProperty(
                glue_run_configuration=datazone.CfnDataSource.GlueRunConfigurationInputProperty(
                    relational_filter_configurations=[
                        datazone.CfnDataSource.RelationalFilterConfigurationProperty(
                            database_name=f"{environment_name}_pub_db"
                        )
                    ]
                )
            ),
            enable_setting="ENABLED",
            recommendation=datazone.CfnDataSource.RecommendationConfigurationProperty(
                enable_business_name_generation=True
            ),
            schedule=datazone.CfnDataSource.ScheduleConfigurationProperty(
                schedule=data_source_schedule,
                timezone="UTC"
            )
        )

        # Add Deletion Policy to the Data Source
        datazone_data_source.cfn_options.deletion_policy = CfnDeletionPolicy.DELETE

        datazone_data_source.add_dependency(datazone_environment)

        # Register S3 data source location in Lake Formation
        register_s3_location = lakeformation.CfnResource(
            self,
            "RegisterS3Location",
            resource_arn=s3_data_source.bucket_arn,
            role_arn=domain_exec_role.role_arn,
            use_service_linked_role=False
        )
        # Ensure the DataZone environment is created before registering the S3 location
        register_s3_location.node.add_dependency(datazone_environment)

        # Create a dedicated IAM Role for the Glue Crawler
        crawler_role_name = f"GlueCrawlerRole-{project_name}-{environment_name}"[:64]
        crawler_role = iam.Role(
            self,
            "GlueCrawlerRole",
            role_name=crawler_role_name,
            assumed_by=iam.ServicePrincipal("glue.amazonaws.com")
        )

        # Attach managed policy required by Glue
        crawler_role.add_managed_policy(iam.ManagedPolicy.from_aws_managed_policy_name("service-role/AWSGlueServiceRole"))

        # Inline policy for S3 access
        crawler_role.add_to_policy(iam.PolicyStatement(
            actions=[
                "s3:GetObject",
                "s3:PutObject",
                "s3:DeleteObject",
                "s3:ListBucket"
            ],
            resources=[
                s3_data_source.bucket_arn,
                f"{s3_data_source.bucket_arn}/*"
            ]
        ))

        # Inline policy for Glue database and table access
        crawler_role.add_to_policy(iam.PolicyStatement(
            actions=["glue:GetDatabase", "glue:GetTables", "glue:CreateTable", "glue:UpdateTable"],
            resources=[
                self.format_arn(
                    service="glue",
                    resource="database",
                    resource_name=f"{environment_name}_pub_db"
                ),
                self.format_arn(
                    service="glue",
                    resource="table",
                    resource_name=f"{environment_name}_pub_db/*"
                )
            ]
        ))

        # Inline policy for Lake Formation access
        crawler_role.add_to_policy(iam.PolicyStatement(
            actions=["lakeformation:GetDataAccess"],
            resources=["*"]
        ))

        # Inline policy for logging
        crawler_role.add_to_policy(iam.PolicyStatement(
            actions=[
                "logs:CreateLogGroup",
                "logs:CreateLogStream",
                "logs:PutLogEvents"
            ],
            resources=[f"arn:aws:logs:{self.region}:{self.account}:log-group:/aws-glue/*"]
        ))

        # Tags for crawler role
        Tags.of(crawler_role).add("Project", project_name)
        Tags.of(crawler_role).add("Environment", environment_name)

        # Grant Lake Formation permissions to the Glue Crawler role for the S3 data location
        lakeformation_permissions_crawler_s3 = lakeformation.CfnPermissions(
            self,
            "LakeFormationPermissionsCrawlerS3",
            data_lake_principal=lakeformation.CfnPermissions.DataLakePrincipalProperty(
                data_lake_principal_identifier=crawler_role.role_arn
            ),
            resource=lakeformation.CfnPermissions.ResourceProperty(
                data_location_resource=lakeformation.CfnPermissions.DataLocationResourceProperty(
                    s3_resource=s3_data_source.bucket_arn
                )
            ),
            permissions=["DATA_LOCATION_ACCESS"],
            permissions_with_grant_option=[]
        )
        # Ensure S3 location is registered before granting permissions
        lakeformation_permissions_crawler_s3.node.add_dependency(register_s3_location)

        # Grant Lake Formation permissions to the Glue Crawler role for the public database
        lakeformation_permissions_crawler_db = lakeformation.CfnPermissions(
            self,
            "LakeFormationPermissionsCrawlerDB",
            data_lake_principal=lakeformation.CfnPermissions.DataLakePrincipalProperty(
                data_lake_principal_identifier=crawler_role.role_arn
            ),
            resource=lakeformation.CfnPermissions.ResourceProperty(
                database_resource=lakeformation.CfnPermissions.DatabaseResourceProperty(
                    name=f"{environment_name}_pub_db"
                )
            ),
            permissions=["ALL"],  # Consider specifying granular permissions in production
            permissions_with_grant_option=[]
        )
        # Ensure database exists before granting permissions
        lakeformation_permissions_crawler_db.node.add_dependency(datazone_environment)

        # Glue Crawler Configuration to update partitions
        crawler_configuration = json.dumps({
            "Version": 1.0,
            "CrawlerOutput": {
                "Partitions": {
                    "AddOrUpdateBehavior": "InheritFromTable"
                }
            }
        })

        # Glue Crawler definition using the new crawler role
        glue_crawler = glue.CfnCrawler(
            self,
            "GlueCrawler",
            name=crawler_name,
            role=crawler_role.role_arn,
            database_name=f"{environment_name}_pub_db",
            targets=glue.CfnCrawler.TargetsProperty(
                s3_targets=[glue.CfnCrawler.S3TargetProperty(
                    path=f"s3://{s3_data_source.bucket_name}/"
                )]
            ),
            table_prefix=f"{environment_name}_pub_",
            schema_change_policy=glue.CfnCrawler.SchemaChangePolicyProperty(
                update_behavior="UPDATE_IN_DATABASE",
                delete_behavior="LOG"
            ),
            configuration=crawler_configuration,
            schedule=glue.CfnCrawler.ScheduleProperty(
                schedule_expression=glue_crawler_schedule
            )
        )
        glue_crawler.node.add_dependency(register_s3_location)
        glue_crawler.node.add_dependency(crawler_role)
        glue_crawler.node.add_dependency(lakeformation_permissions_crawler_s3)
        glue_crawler.node.add_dependency(lakeformation_permissions_crawler_db)

        # Lake Formation permissions for public database to domain execution role
        lakeformation_permissions_role_pub = lakeformation.CfnPermissions(
            self,
            "LakeFormationPermissionsRolePub",
            data_lake_principal=lakeformation.CfnPermissions.DataLakePrincipalProperty(
                data_lake_principal_identifier=domain_exec_role.role_arn
            ),
            resource=lakeformation.CfnPermissions.ResourceProperty(
                database_resource=lakeformation.CfnPermissions.DatabaseResourceProperty(
                    name=f"{environment_name}_pub_db"
                )
            ),
            permissions=["ALL"],
            permissions_with_grant_option=["ALL"]
        )
        lakeformation_permissions_role_pub.node.add_dependency(datazone_environment)

        # Lake Formation permissions for subscriber database to domain execution role
        lakeformation_permissions_role_sub = lakeformation.CfnPermissions(
            self,
            "LakeFormationPermissionsRoleSub",
            data_lake_principal=lakeformation.CfnPermissions.DataLakePrincipalProperty(
                data_lake_principal_identifier=domain_exec_role.role_arn
            ),
            resource=lakeformation.CfnPermissions.ResourceProperty(
                database_resource=lakeformation.CfnPermissions.DatabaseResourceProperty(
                    name=f"{environment_name}_sub_db"
                )
            ),
            permissions=["ALL"],
            permissions_with_grant_option=["ALL"]
        )
        lakeformation_permissions_role_sub.node.add_dependency(datazone_environment)

        # Outputs for created resources
        CfnOutput(self, "S3DataSourceBucketName", value=s3_data_source.bucket_name)
        CfnOutput(self, "S3DataZoneSysStoreBucketName", value=s3_datazone_sys.bucket_name)
        CfnOutput(self, "GlueCrawlerName", value=glue_crawler.name)
        CfnOutput(self, "GlueTablePrefix", value=f"{environment_name}_pub_")
        CfnOutput(self, "DataZoneDomainPortalUrl", value=datazone_domain.attr_portal_url)
        CfnOutput(self, "DataZoneDomainId", value=datazone_domain.attr_id)
        CfnOutput(self, "DataZoneExecutionRoleArn", value=domain_exec_role.role_arn)
